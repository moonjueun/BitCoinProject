{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import quantstats as qs\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import os\n",
    "import cv2\n",
    "# import pywt\n",
    "\n",
    "from binance.client import Client\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing (raw -> 성능 좋지 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coin name :  BTCUSDT\n",
      "zeros : 3, plus : 4, minus : 5\n",
      "coin name :  ETHUSDT\n",
      "zeros : 2, plus : 5, minus : 5\n",
      "coin name :  BNBUSDT\n",
      "zeros : 2, plus : 3, minus : 7\n",
      "coin name :  ADAUSDT\n",
      "zeros : 0, plus : 3, minus : 9\n",
      "coin name :  SOLUSDT\n",
      "zeros : 0, plus : 5, minus : 7\n",
      "coin name :  XRPUSDT\n",
      "zeros : 2, plus : 3, minus : 7\n",
      "coin name :  DOTUSDT\n",
      "zeros : 0, plus : 3, minus : 9\n",
      "coin name :  AVAXUSDT\n",
      "zeros : 1, plus : 1, minus : 10\n",
      "coin name :  DOGEUSDT\n",
      "zeros : 1, plus : 5, minus : 6\n",
      "coin name :  LTCUSDT\n",
      "zeros : 1, plus : 5, minus : 6\n",
      "coin name :  WLDUSDT\n",
      "zeros : 0, plus : 2, minus : 10\n"
     ]
    }
   ],
   "source": [
    "# Binance API 키 설정\n",
    "API_KEY = 'nu1AV1q04nvVCGcZpxmYYM4AJovxghU644sTP5Q59Xo4T5tjwsBwLEp4viXQU4om'\n",
    "API_SECRET = 'yruAAb0iJ1mwz8hsxVIrklph4YWW82Uhb1vlPdiQ1pNMV9EJWOjV1MY0UPHPp7ev'\n",
    "\n",
    "client = Client(API_KEY, API_SECRET)\n",
    "\n",
    "# 한국 시간과 UTC와의 시차\n",
    "# utc_offset = timedelta(hours=9)\n",
    "utc_offset = timedelta(hours=0)\n",
    "\n",
    "# 데이터 수집 기간 설정 (3개월)\n",
    "end_date = datetime.now(pytz.timezone('Asia/Seoul'))  # 현재 한국 시간\n",
    "start_date = end_date - timedelta(days=10)  # 365일 전까지의 데이터 수집\n",
    "\n",
    "# start_date = end_date - timedelta(days=90)  # 365일 전까지의 데이터 수집\n",
    "\n",
    "# UTC 시간을 한국 시간으로 변환하여 데이터 수집\n",
    "start_date_utc = start_date.astimezone(pytz.utc)\n",
    "end_date_utc = end_date.astimezone(pytz.utc)\n",
    "\n",
    "\n",
    "################### Data Load ###################\n",
    "top_coins_usdt = [\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\", \"ADAUSDT\", \"SOLUSDT\", \"XRPUSDT\", \"DOTUSDT\", \"AVAXUSDT\", \"DOGEUSDT\", \"LTCUSDT\", \"WLDUSDT\"]\n",
    "\n",
    "save_root = \"/home/wondong/coin/Timeseries/datasets\"\n",
    "\n",
    "if not os.path.exists(save_root):\n",
    "    os.mkdir(save_root)\n",
    "\n",
    "\n",
    "for coin in top_coins_usdt:\n",
    "    # 데이터 수집\n",
    "    symbol = coin  # 비트코인/달러 페어\n",
    "    interval = Client.KLINE_INTERVAL_5MINUTE  # 5분 주기 데이터\n",
    "    candle = client.get_historical_klines(symbol, interval, start_date_utc.strftime(\"%Y-%m-%d %H:%M:%S\"), end_date_utc.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    # 데이터를 데이터프레임으로 변환\n",
    "    columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']\n",
    "    df = pd.DataFrame(candle, columns=columns)\n",
    "\n",
    "    # timestamp 열을 날짜 형식으로 변환 (UTC 시간에서 한국 시간으로 변환)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Seoul')\n",
    "\n",
    "    # 필요한 열만 선택\n",
    "    df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    df.reset_index()\n",
    "\n",
    "    df.columns = [\"Date\", \"Open\", \"High\", \"Low\",\"Close\", \"Volume\"]\n",
    "\n",
    "    df['Open'] = df['Open'].astype(np.float64)\n",
    "    df['High'] = df['High'].astype(np.float64)\n",
    "    df['Low'] = df['Low'].astype(np.float64)\n",
    "    df['Close'] = df['Close'].astype(np.float64)\n",
    "    df['Volume'] = df['Volume'].astype(np.float64)\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df_ = df.set_index(\"Date\")\n",
    "\n",
    "    \n",
    "    ################### Time Series data to Image Data ###################\n",
    "    df = df_[:].copy()\n",
    "    length = len(df) // 224\n",
    "    test_data = df\n",
    "\n",
    "    mean_data_ = (test_data[\"Low\"].values.astype(np.float64) + test_data[\"High\"].values.astype(np.float64)) / 2\n",
    "    close_data_ = test_data[\"Close\"].values.astype(np.float64)\n",
    "    volume_data_ = test_data[\"Volume\"].values.astype(np.float64)\n",
    "\n",
    "    window_size = 224\n",
    "        \n",
    "    print(\"coin name : \", coin)\n",
    "    \n",
    "    zeros = 0\n",
    "    minus = 0\n",
    "    plus = 0\n",
    "    \n",
    "    for i in range(length):\n",
    "        mean_data = mean_data_[i* window_size : (i+1) * window_size]\n",
    "        \n",
    "        close_data = close_data_[i* window_size : (i+1) * window_size]\n",
    "        close_copy = close_data.copy()\n",
    "        \n",
    "        \n",
    "        volume_data = volume_data_[i* window_size : (i+1) * window_size]\n",
    "        lenth_test = len(volume_data)\n",
    "\n",
    "        if volume_data.all() == 0.:\n",
    "            volume_data = [1e-04] * lenth_test\n",
    "            volume_data = np.array(volume_data)\n",
    "\n",
    "        close_data = np.expand_dims(close_data, axis = 1)\n",
    "        mean_data = np.expand_dims(mean_data, axis = 1)\n",
    "        volume_data = np.expand_dims(volume_data, axis = 1)\n",
    "\n",
    "        total_data = np.concatenate([close_data, mean_data, volume_data], axis = 1)\n",
    "        \n",
    "        \n",
    "        predict_value = close_data_[(i+1) * window_size+1]\n",
    "        \n",
    "        interest_rate = (predict_value / close_copy[-1]) -1\n",
    "        if interest_rate > 0.0005:\n",
    "            gt = 1\n",
    "            plus +=1\n",
    "        elif interest_rate < -0.0005:\n",
    "            gt = -1\n",
    "            minus +=1\n",
    "        else:\n",
    "            gt = 0\n",
    "            zeros += 1\n",
    "\n",
    "        \n",
    "        save_dir = save_root + f\"/{coin}\" \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_path = save_dir + \"/\" +\"time_series_\" + str(i) +\".npy\"\n",
    "        # save_img = save_dir + \"/\" +\"image_\" + str(i) +\".png\"\n",
    "        save_gt_path = save_dir + \"/\" +\"gt_\" + str(i) +\".npy\"\n",
    "\n",
    "        np.save(save_path, total_data)\n",
    "        # # cv2.imwrite(save_img, viz_img)\n",
    "        np.save(save_gt_path, gt)\n",
    "        \n",
    "        \n",
    "    print(\"zeros : {}, plus : {}, minus : {}\".format(zeros, plus, minus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Data Processing (normalization and log-scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "root_path = \"../version3_ViT/dataset\"\n",
    "\n",
    "coin_names = os.listdir(root_path)\n",
    "\n",
    "\n",
    "coin_list = []\n",
    "gt_list = []\n",
    "\n",
    "\n",
    "for coin in coin_names :\n",
    "    coin_paths = sorted(glob(root_path + \"/\" + coin +\"/time_series_*.npy\"))\n",
    "    gt_paths = sorted(glob(root_path + \"/\" + coin +\"/gt_*.npy\"))\n",
    "    \n",
    "    coin_list.extend(coin_paths)\n",
    "    gt_list.extend(gt_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df =  pd.DataFrame({\"original_paths\" : coin_list, \n",
    "#                    \"coin_gt_paths\" : gt_list})\n",
    "# df.to_csv(\"/home/wondong/coin/Timeseries/train_dataset.csv\")\n",
    "df = pd.read_csv('../version3_ViT/dataset/train_dataset.csv', index_col=0)\n",
    "df.rename({'coin_img_paths': 'original_paths'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_path, is_train):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        img_path = df['original_paths'].tolist()\n",
    "        gt_path = df['coin_gt_paths'].tolist()\n",
    "\n",
    "        self.train_data_path, self.test_data_path, \\\n",
    "            self.train_gt_path, self.test_gt_path = \\\n",
    "                train_test_split(img_path, gt_path, random_state=4715, test_size=0.3)\n",
    "        \n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_train == True:\n",
    "            return len(self.train_data_path)\n",
    "        else:\n",
    "            return len(self.test_data_path)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            data_path = self.train_data_path[idx]\n",
    "            gt_path = self.train_gt_path[idx]\n",
    "\n",
    "            original_data = np.load(data_path)\n",
    "            gt = np.load(gt_path)\n",
    "            \n",
    "            if gt == -1:\n",
    "                gt =  np.array([1, 0, 0])\n",
    "            elif gt == 0:\n",
    "                gt = np.array([0, 1, 0])\n",
    "            elif gt == 1:\n",
    "                gt = np.array([0, 0, 1])\n",
    "\n",
    "            return original_data, gt\n",
    "        \n",
    "        else:\n",
    "            data_path = self.train_data_path[idx]\n",
    "            gt_path = self.train_gt_path[idx]\n",
    "\n",
    "            original_data = np.load(data_path)\n",
    "            gt = np.load(gt_path)\n",
    "            if gt == -1:\n",
    "                gt =  np.array([1, 0, 0])\n",
    "            elif gt == 0:\n",
    "                gt = np.array([0, 1, 0])\n",
    "            elif gt == 1:\n",
    "                gt = np.array([0, 0, 1])\n",
    "\n",
    "            return original_data, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset('../version3_ViT/dataset/train_dataset.csv', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_channel=3, output_channel=3, batch_size=16):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Layer\n",
    "        self.lstm1 = nn.LSTM(input_size=self.input_channel, hidden_size=128, num_layers=3, bidirectional=False, batch_first=True)\n",
    "        self.activation1 = nn.GELU()\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=3, bidirectional=False, batch_first=True)\n",
    "        self.activation2 = nn.GELU()\n",
    "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=self.output_channel, num_layers=3, bidirectional=False, batch_first=True)\n",
    "        self.activation3 = nn.GELU()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.input_channel * 224, out_features=256)\n",
    "        self.activation4 = nn.GELU()\n",
    "        self.fc2 = nn.Linear(in_features= 256, out_features=128)\n",
    "        self.activation5 = nn.GELU()\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=3)\n",
    "        self.activation6 = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, _, _ = x.shape\n",
    "        x, hidden_tuple1 = self.lstm1(x)\n",
    "        x = self.activation1(x)\n",
    "        x, hidden_tuple2 = self.lstm2(x)\n",
    "        x = self.activation2(x)\n",
    "        x, hidden_tuple3 = self.lstm3(x)\n",
    "        x = self.activation3(x)\n",
    "\n",
    "        x = x.reshape(batch, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation5(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation6(x)\n",
    "        return x\n",
    "        \n",
    "model = CustomLSTM(input_channel=3, output_channel=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.3282e+02, 1.3273e+02, 5.0191e+04],\n",
      "         [1.3275e+02, 1.3270e+02, 2.3227e+04],\n",
      "         [1.3266e+02, 1.3274e+02, 2.1581e+04],\n",
      "         ...,\n",
      "         [1.3424e+02, 1.3390e+02, 7.2159e+04],\n",
      "         [1.3464e+02, 1.3432e+02, 5.3251e+04],\n",
      "         [1.3489e+02, 1.3448e+02, 3.8701e+04]],\n",
      "\n",
      "        [[7.5240e-01, 7.5085e-01, 2.5102e+06],\n",
      "         [7.5130e-01, 7.5430e-01, 2.5556e+06],\n",
      "         [7.4780e-01, 7.5025e-01, 1.2452e+06],\n",
      "         ...,\n",
      "         [7.4240e-01, 7.4095e-01, 6.8422e+05],\n",
      "         [7.4100e-01, 7.4135e-01, 3.3269e+05],\n",
      "         [7.4180e-01, 7.3990e-01, 8.6892e+05]],\n",
      "\n",
      "        [[4.1550e+01, 4.1625e+01, 1.3853e+04],\n",
      "         [4.1530e+01, 4.1595e+01, 8.1458e+03],\n",
      "         [4.1400e+01, 4.1465e+01, 1.5175e+04],\n",
      "         ...,\n",
      "         [4.8150e+01, 4.7960e+01, 4.3207e+04],\n",
      "         [4.8030e+01, 4.8050e+01, 1.7513e+04],\n",
      "         [4.7840e+01, 4.7935e+01, 1.2655e+04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.4698e+02, 1.4712e+02, 1.0478e+04],\n",
      "         [1.4695e+02, 1.4707e+02, 1.1193e+04],\n",
      "         [1.4683e+02, 1.4695e+02, 1.0656e+04],\n",
      "         ...,\n",
      "         [1.5167e+02, 1.5169e+02, 2.7254e+04],\n",
      "         [1.5012e+02, 1.5087e+02, 3.3099e+04],\n",
      "         [1.5020e+02, 1.5028e+02, 1.7626e+04]],\n",
      "\n",
      "        [[4.1380e+02, 4.1450e+02, 5.1863e+03],\n",
      "         [4.1460e+02, 4.1405e+02, 2.0286e+03],\n",
      "         [4.1520e+02, 4.1485e+02, 2.9990e+03],\n",
      "         ...,\n",
      "         [4.2050e+02, 4.2065e+02, 4.9910e+03],\n",
      "         [4.2070e+02, 4.2080e+02, 2.7861e+03],\n",
      "         [4.2030e+02, 4.2040e+02, 2.0751e+03]],\n",
      "\n",
      "        [[1.6263e-01, 1.6318e-01, 1.8665e+07],\n",
      "         [1.6257e-01, 1.6308e-01, 2.0312e+07],\n",
      "         [1.6417e-01, 1.6338e-01, 1.7341e+07],\n",
      "         ...,\n",
      "         [1.6550e-01, 1.6519e-01, 4.7931e+06],\n",
      "         [1.6553e-01, 1.6543e-01, 4.9759e+06],\n",
      "         [1.6595e-01, 1.6578e-01, 1.0117e+07]]], device='cuda:0')\n",
      "tensor([[[5.0950e+01, 5.0695e+01, 9.8776e+04],\n",
      "         [5.0740e+01, 5.0765e+01, 5.6313e+04],\n",
      "         [5.0640e+01, 5.0720e+01, 2.6179e+04],\n",
      "         ...,\n",
      "         [5.4230e+01, 5.4130e+01, 8.8220e+03],\n",
      "         [5.4480e+01, 5.4395e+01, 2.6192e+04],\n",
      "         [5.4320e+01, 5.4370e+01, 2.0017e+04]],\n",
      "\n",
      "        [[1.7307e-01, 1.7389e-01, 2.0065e+07],\n",
      "         [1.7298e-01, 1.7315e-01, 1.2593e+07],\n",
      "         [1.7458e-01, 1.7375e-01, 1.3535e+07],\n",
      "         ...,\n",
      "         [1.8646e-01, 1.8664e-01, 2.9830e+07],\n",
      "         [1.8828e-01, 1.8758e-01, 2.2454e+07],\n",
      "         [1.8735e-01, 1.8787e-01, 1.6193e+07]],\n",
      "\n",
      "        [[8.8900e+01, 8.9030e+01, 2.7404e+03],\n",
      "         [8.8950e+01, 8.8885e+01, 1.7140e+03],\n",
      "         [8.9320e+01, 8.9135e+01, 1.1868e+03],\n",
      "         ...,\n",
      "         [8.4650e+01, 8.5285e+01, 3.4458e+04],\n",
      "         [8.5760e+01, 8.5185e+01, 1.4686e+04],\n",
      "         [8.6190e+01, 8.5985e+01, 9.0943e+03]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wondong/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1043773889541626, 1.105668544769287]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "train_dataset = CustomDataset(\"/home/wondong/coin/Timeseries/train_dataset.csv\", True)\n",
    "val_dataset = CustomDataset(\"/home/wondong/coin/Timeseries/train_dataset.csv\", False)\n",
    "\n",
    "epoches = 100\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cup\" \n",
    "\n",
    "### 1. model 정의\n",
    "model = CustomLSTM()\n",
    "model = model.to(device)\n",
    "## 2. optimizer 정의\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 1e-04, betas = (0.9, 0.999))\n",
    "## 3. Scheduler 정의\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "## 4. criterion 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = CustomDataset(\"/home/wondong/coin/Timeseries/train_dataset.csv\", True)\n",
    "test_dataset = CustomDataset(\"/home/wondong/coin/Timeseries/train_dataset.csv\", False) \n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=4715)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    train_losses = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n",
    "        train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "        train_subset_loader = DataLoader(train_subset, batch_size = 16, shuffle = True)\n",
    "        val_subset = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "        val_subset_loader = DataLoader(val_subset, batch_size = 16, shuffle = False)\n",
    "\n",
    "\n",
    "        train_loss_list = []\n",
    "        for (t_idx), (train_data, train_gt) in enumerate(train_subset_loader):\n",
    "            train_data = train_data.to(device)\n",
    "            train_data = train_data.to(torch.float)\n",
    "            train_gt = train_gt.to(device)\n",
    "            train_gt = train_gt.to(torch.float)\n",
    "\n",
    "\n",
    "            train_prediction = model(train_data)\n",
    "\n",
    "            train_loss = criterion(train_prediction, train_gt)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(train_loss.item())\n",
    "\n",
    "            break\n",
    "        val_loss_list = []\n",
    "        with torch.no_grad():\n",
    "            for (v_idx), (val_data, val_gt) in enumerate(val_subset_loader):\n",
    "                val_data = val_data.to(device)\n",
    "                val_data = val_data.to(torch.float)\n",
    "                val_gt = val_gt.to(device)\n",
    "                val_gt = val_gt.to(torch.float)\n",
    "\n",
    "                print(val_data)\n",
    "\n",
    "\n",
    "                val_prediction = model(val_data)\n",
    "\n",
    "                val_loss = criterion(val_prediction, val_gt)\n",
    "                val_loss_list.append(val_loss.item())\n",
    "\n",
    "    # for val_idx, (val_data, val_gt) in enumerate(val_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        break\n",
    "    break\n",
    "val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 데이터셋을 불러오고 초기화\n",
    "dataset = CustomDataset(csv_path='/home/wondong/coin/Timeseries/train_dataset.csv', is_train=True)\n",
    "\n",
    "# KFold 객체 생성\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=4715)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 224, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 224, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 224, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([16, 224, 3])\n",
      "torch.Size([16, 3])\n",
      "torch.Size([9, 224, 3])\n",
      "torch.Size([9, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(csv_path='/home/wondong/coin/Timeseries/train_dataset.csv', is_train=True)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=4715)\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    train_subset_loader = DataLoader(train_subset, batch_size = 16, shuffle = True)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "    val_subset_loader = DataLoader(val_subset, batch_size = 16, shuffle = False)\n",
    "\n",
    "    for train_idx, (train_data, train_gt) in enumerate(train_subset_loader):\n",
    "        \n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 30])\n",
      "torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.randn([20, 30])\n",
    "layer1 = nn.Linear(30, 10)\n",
    "\n",
    "print(test_input.shape)\n",
    "output = layer1(test_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wondong/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3584x3 and 256x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 16, 3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sample\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[79], line 24\u001b[0m, in \u001b[0;36mCustomLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m x, hidden_tuple3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm3(x)\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation3(x)\n\u001b[0;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x3 and 256x3)"
     ]
    }
   ],
   "source": [
    "sample = model(val_data) # 16, 3\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = output[0]\n",
    "hidden_state, cell_state = output[1]\n",
    "\n",
    "hidden_state.shape # 1, 16, 20\n",
    "cell_state.shape # 1, 16, 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 20])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state.shape # 1, 16, 20\n",
    "cell_state.shape # 1, 16, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 224, 3])\n",
      "torch.Size([16, 224, 20])\n"
     ]
    }
   ],
   "source": [
    "#### input shape : 1, 224, 3\n",
    "print(val_data.shape)\n",
    "## LSTM(3, 20)\n",
    "\n",
    "#### output shape : 1, 224, 20\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 224, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModifiedDinoVisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedDinoVisionTransformer, self).__init__()\n",
    "        \n",
    "        # 기존 DinoVisionTransformer 모델 로드\n",
    "        self.dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "        \n",
    "        # 추가 레이어 정의\n",
    "        self.additional_layer = nn.Linear(384, 3)  # 입력 크기: 384, 출력 크기: 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dino_model(x)\n",
    "        x = self.additional_layer(x)\n",
    "        x = F.softmax(x, dim=-1)  # Softmax 레이어 추가\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/wondong/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /home/wondong/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n",
      "100%|██████████| 84.2M/84.2M [00:12<00:00, 7.20MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gpqjfukg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>████▇▆▆▆▆▅▄▄▄▄▃▄▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▁▁▂▁▁</td></tr><tr><td>validation loss</td><td>▁▁▁▂▂▂▃▂▃▃▃▃▄▄▅▃▅▆▅▅▅▆▆▆▆▆▆▆▆▇▆▆▆▇█▇▆▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>0.60625</td></tr><tr><td>validation loss</td><td>1.13369</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scalogram_1</strong> at: <a href='https://wandb.ai/sungkyunkwan_/crypo/runs/gpqjfukg' target=\"_blank\">https://wandb.ai/sungkyunkwan_/crypo/runs/gpqjfukg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240306_013950-gpqjfukg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gpqjfukg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wondong/coin/DinoV2/wandb/run-20240306_113459-5hw686pj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sungkyunkwan_/crypo/runs/5hw686pj' target=\"_blank\">scalogram_2</a></strong> to <a href='https://wandb.ai/sungkyunkwan_/crypo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sungkyunkwan_/crypo' target=\"_blank\">https://wandb.ai/sungkyunkwan_/crypo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sungkyunkwan_/crypo/runs/5hw686pj' target=\"_blank\">https://wandb.ai/sungkyunkwan_/crypo/runs/5hw686pj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Train Loss :  1.0463710927088326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3000 [03:44<186:52:18, 224.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0487006805900834\n",
      "\n",
      "##################################################\n",
      "Wow New Model Pop up :  1.0487006805900834\n",
      "##################################################\n",
      "Mean of Train Loss :  1.0361174785762752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3000 [07:28<186:34:18, 224.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0431912633088918\n",
      "\n",
      "##################################################\n",
      "Wow New Model Pop up :  1.0431912633088918\n",
      "##################################################\n",
      "Mean of Train Loss :  1.0276414332039858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/3000 [11:12<186:40:37, 224.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0426146421167586\n",
      "\n",
      "##################################################\n",
      "Wow New Model Pop up :  1.0426146421167586\n",
      "##################################################\n",
      "Mean of Train Loss :  1.018670498777967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/3000 [14:56<186:34:49, 224.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.042046935385109\n",
      "\n",
      "##################################################\n",
      "Wow New Model Pop up :  1.042046935385109\n",
      "##################################################\n",
      "Mean of Train Loss :  1.0037640946720718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/3000 [18:41<186:34:50, 224.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0405482369610386\n",
      "\n",
      "##################################################\n",
      "Wow New Model Pop up :  1.0405482369610386\n",
      "##################################################\n",
      "Mean of Train Loss :  0.9844164379146121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/3000 [22:25<186:27:14, 224.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.04343225252934\n",
      "Mean of Train Loss :  0.9571170568466186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3000 [26:09<186:20:15, 224.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0552664136784708\n",
      "Mean of Train Loss :  0.9322141042543114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/3000 [29:53<186:15:04, 224.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0535796035049307\n",
      "Mean of Train Loss :  0.9136865070106787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/3000 [33:37<186:08:54, 224.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.058893679553627\n",
      "Mean of Train Loss :  0.9041679062974562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/3000 [37:20<186:01:06, 223.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0550479545043066\n",
      "Mean of Train Loss :  0.9014626585015463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/3000 [41:04<185:54:59, 223.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0550479545043066\n",
      "Mean of Train Loss :  0.9018262796445724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/3000 [44:48<185:47:55, 223.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0559483657025883\n",
      "Mean of Train Loss :  0.9021316117102947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/3000 [48:32<185:42:50, 223.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0571822108875992\n",
      "Mean of Train Loss :  0.8995947263656406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/3000 [52:15<185:36:06, 223.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0574423680957565\n",
      "Mean of Train Loss :  0.8936219563177966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/3000 [55:59<185:29:16, 223.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0650682798308184\n",
      "Mean of Train Loss :  0.8839102682717349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/3000 [59:43<185:26:45, 223.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.060422647712577\n",
      "Mean of Train Loss :  0.8677491836591598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/3000 [1:03:27<185:24:47, 223.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0724250974818172\n",
      "Mean of Train Loss :  0.8523658724006163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/3000 [1:07:10<185:17:57, 223.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.075333234336641\n",
      "Mean of Train Loss :  0.8322419935410176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/3000 [1:10:54<185:15:54, 223.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0762943605072477\n",
      "Mean of Train Loss :  0.816101586818695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/3000 [1:14:38<185:12:39, 223.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0878328423724215\n",
      "Mean of Train Loss :  0.7922155904113699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/3000 [1:18:21<185:08:05, 223.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0903859742176838\n",
      "Mean of Train Loss :  0.769265995550593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/3000 [1:22:13<187:02:45, 226.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Val Loss :  1.0839455153188136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/3000 [1:22:20<185:44:58, 224.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(train_pred, train_gt)\n\u001b[1;32m     52\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss\n\u001b[0;32m---> 53\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     54\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "train_dataset = CustomDataset(\"/home/wondong/coin/DinoV2/dataset_scelo/train_dataset.csv\", True)\n",
    "test_dataset = CustomDataset(\"/home/wondong/coin/DinoV2/dataset_scelo/train_dataset.csv\", False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size = 32)\n",
    "\n",
    "model = ModifiedDinoVisionTransformer()\n",
    "\n",
    "epoch_size = 3000\n",
    "device =  \"cuda\" if torch.cuda.is_available() == True else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# AdamW 옵티마이저 정의\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.000001)\n",
    "\n",
    "# 코사인 스케줄러 정의\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "save_path = \"/home/wondong/coin/DinoV2/checkpoints2\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "save_checkpoints = save_path + \"/best_aug.pt\"\n",
    "min_loss = 100\n",
    "\n",
    "\n",
    "wandb.init(project=\"crypo\", name=\"scalogram_2\")\n",
    "\n",
    "for epoch in tqdm(range(epoch_size)):\n",
    "    train_losses = []\n",
    "    for train_idx, (train_img, train_gt) in enumerate(train_dataloader):\n",
    "\n",
    "        train_img = train_img.to(device)\n",
    "        train_img = train_img.to(dtype = torch.float)\n",
    "        \n",
    "        train_gt = train_gt.to(device)\n",
    "        train_gt = train_gt.to(dtype = torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_pred = model(train_img)\n",
    "\n",
    "        train_loss = criterion(train_pred, train_gt)\n",
    "        train_loss = train_loss\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses_np = np.array(train_losses)\n",
    "    mean_train_loss = np.mean(train_losses_np)\n",
    "\n",
    "    print(\"Mean of Train Loss : \", mean_train_loss)\n",
    "\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for val_idx, (val_img, val_gt) in enumerate(test_dataloader):\n",
    "\n",
    "            val_img = val_img.to(device)\n",
    "            val_img = val_img.to(dtype = torch.float)\n",
    "        \n",
    "            val_gt = val_gt.to(device)\n",
    "            val_gt = val_gt.to(dtype = torch.float)\n",
    "\n",
    "            val_pred = model(val_img)\n",
    "\n",
    "            val_loss = criterion(val_pred, val_gt)\n",
    "            val_loss = val_loss \n",
    "            val_losses.append(val_loss.item())\n",
    "  \n",
    "\n",
    "    val_losses_np = np.array(val_losses)\n",
    "    mean_val_loss = np.mean(val_losses_np)\n",
    "\n",
    "\n",
    "    print(\"Mean of Val Loss : \", mean_val_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    wandb.log({\"train loss\": mean_train_loss, \"validation loss\": mean_val_loss})\n",
    "\n",
    "    if min_loss > mean_val_loss:\n",
    "        min_loss = mean_val_loss\n",
    "        print(\"\")\n",
    "        print(\"#\"*50)\n",
    "        print(\"Wow New Model Pop up : \", min_loss)\n",
    "        print(\"#\"*50)\n",
    "        \n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        \n",
    "        torch.save(model.state_dict(), save_checkpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
